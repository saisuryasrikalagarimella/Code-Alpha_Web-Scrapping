import requests
from bs4 import BeautifulSoup
import csv

base_url = 'https://quotes.toscrape.com/page/{}/'
all_quotes = []

# Loop through pages until no more quotes are found
page = 1
while True:
    url = base_url.format(page)
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    quotes = soup.find_all('div', class_='quote')
    if not quotes:
        break  # No more quotes, exit loop

    for quote in quotes:
        text = quote.find('span', class_='text').text
        author = quote.find('small', class_='author').text
        tags = [tag.text for tag in quote.find_all('a', class_='tag')]
        all_quotes.append([text, author, ', '.join(tags)])

    print(f'Scraped page {page}')
    page += 1
